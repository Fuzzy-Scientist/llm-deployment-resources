{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MLflow Integration for Model Training and Tracking\n",
    "\n",
    "In this notebook, we're integrating MLflow into a machine learning workflow to track and manage experiments effectively. We're focusing on a text classification task using the DistilBert model, emphasizing the importance of experiment tracking, model management, and operational efficiency - core themes of our course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "- Dynamically set up and log parameters to MLflow\n",
    "- Understand the purpose and application of each step in the context of MLflow and MLOps principles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Ensure all necessary libraries are installed and imported for our workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imports\n",
    "\n",
    "Import the necessary libraries, focusing on MLflow for tracking, PyTorch for model training, and Transformers for our NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "import os\n",
    "import mlflow\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters as an Object\n",
    "\n",
    "By defining parameters as a dictionary, we can easily iterate through them when logging to MLflow. This method streamlines the process and adheres to best practices in code maintainability and scalability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model_name': 'distilbert-base-cased',\n",
    "    'learning_rate': 5e-5,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 1,\n",
    "    'dataset_name': 'ag_news',\n",
    "    'task_name': 'sequence_classification',\n",
    "    'log_steps': 100,\n",
    "    'max_seq_length': 128,\n",
    "    'output_dir': 'models/distilbert-base-uncased-ag_news',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MLflow Setup\n",
    "\n",
    "Setting up MLflow is crucial for tracking our experiments, parameters, and results, allowing us to manage and compare different runs effectively - a practice that aligns with the MLOps goal of systematic and efficient model management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5005\")\n",
    "mlflow.set_experiment(f\"{params['task_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Dataset\n",
    "\n",
    "We're using a well-known NLP dataset to ensure reproducibility and comparability. The preprocessing step is crucial for converting raw text into a format that our model can understand, highlighting the importance of data preparation in the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "dataset = load_dataset(params['dataset_name'], params['task_name'])\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(params['model_name'])\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=params['max_seq_length'])\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"].shuffle().select(range(20_000)).map(tokenize, batched=True)\n",
    "test_dataset = dataset[\"test\"].shuffle().select(range(2_000)).map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch and create data loaders\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "# get the labels\n",
    "labels = dataset[\"train\"].features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model Initialization\n",
    "\n",
    "Initializing the model is a foundational step, showcasing the practical application of a pre-trained NLP model for a specific task - reflecting the course's focus on real-world applicability of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(params['model_name'], num_labels=len(labels))\n",
    "model.config.id2label = {i: label for i, label in enumerate(labels)}\n",
    "params['id2label'] = model.config.id2label\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Setup\n",
    "\n",
    "Choosing the right optimizer and learning rate is vital for effective model training. It demonstrates the importance of hyperparameter tuning, a key concept in achieving optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "Evaluating the model on a separate test set helps us understand its performance on unseen data, highlighting the concept of generalization which is crucial for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, masks, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "            logits = outputs.logits\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate Evaluation Metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The training loop is where the actual model training happens. Logging metrics and parameters at each step is crucial for tracking the model's progress, understanding its behavior, and making informed decisions - core aspects of the MLOps lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow Run\n",
    "with mlflow.start_run(run_name=f\"{params['model_name']}-{params['dataset_name']}\") as run:\n",
    "\n",
    "    # Log all parameters at once\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    with tqdm(total=params['num_epochs'] * len(train_loader), desc=f\"Epoch [1/{params['num_epochs']}] - (Loss: N/A) - Steps\") as pbar:\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            running_loss = 0.0\n",
    "            for i, batch in enumerate(train_loader, 0):\n",
    "                inputs, masks, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                if i and i % params['log_steps'] == 0:\n",
    "                    avg_loss = running_loss / params['log_steps']\n",
    "\n",
    "                    pbar.set_description(f\"Epoch [{epoch + 1}/{params['num_epochs']}] - (Loss: {avg_loss:.3f}) - Steps\")\n",
    "                    mlflow.log_metric(\"loss\", avg_loss, step=epoch * len(train_loader) + i)\n",
    "                    \n",
    "                    running_loss = 0.0\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Evaluate Model\n",
    "            accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
    "            print(f\"Epoch {epoch + 1} Metrics: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metrics({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}, step=epoch)\n",
    "\n",
    "\n",
    "    # Log model to MLflow through built-in PyTorch method\n",
    "    # mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    # Log model to MLflow through custom method\n",
    "    os.makedirs(params['output_dir'], exist_ok=True)\n",
    "    model.save_pretrained(params['output_dir'])\n",
    "    tokenizer.save_pretrained(params['output_dir'])\n",
    "\n",
    "    mlflow.log_artifacts(params['output_dir'], artifact_path=\"model\")\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    mlflow.register_model(model_uri, \"agnews-transformer\")\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
