{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Integration for Model Serving and Registry Management\n",
    "\n",
    "In this notebook, we delve into advanced aspects of MLflow, focusing on model serving, inference, and the management of model versions in the MLflow model registry. Our goal is to demonstrate how MLflow supports the operational phase of the machine learning lifecycle, which includes serving models for inference and efficiently managing multiple versions of models.\n",
    "\n",
    "We will explore the practical application of these concepts using a text classification model. This will include loading models for inference, performing predictions, managing different versions of models, and understanding how to transition models through various stages in the model lifecycle. These skills are essential for operational efficiency and effective model management in real-world machine learning applications, aligning with the core themes of our course on MLops and experiment tracking.\n",
    "\n",
    "\n",
    "### Objective:\n",
    "* Loading and Serving Models\n",
    "* Inference with the Model\n",
    "* Managing Model Versions\n",
    "* Deleting Models and Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Ensure all necessary libraries are installed and imported for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import necessary libraries focusing on MLflow for model retrieval, PyTorch for model operations, and Transformers for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Mlflow Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Model from MLflow\n",
    "\n",
    "In this step, we'll explore two methods to retrieve our trained model from MLflow. Understanding the nuances of each method is key to making an informed choice in a real-life scenario based on the requirements and constraints of your deployment environment.\n",
    "\n",
    "#### Method 1: Using the Built-in PyTorch Loader\n",
    "\n",
    "This method is straightforward and uses MLflow's built-in functionality to load PyTorch models. It's user-friendly and works well when you're working within a PyTorch-centric workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific model version\n",
    "model_name = \"agnews_pt_classifier\"\n",
    "model_version = \"1\"  # or \"production\", \"staging\"\n",
    "\n",
    "\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "model = mlflow.pytorch.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Here, we define the `predict` function to perform inference using the loaded model. This function takes a list of texts, tokenizes them using a pre-trained tokenizer, and then feeds them into the model. The output is the model's prediction, which can be used for various applications such as text classification, sentiment analysis, etc. This step is crucial in demonstrating how a trained model can be utilized for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(texts, model, tokenizer):\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Pass the inputs to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert predictions to text labels\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    predictions = [model.config.id2label[prediction] for prediction in predictions]\n",
    "\n",
    "    # Print predictions\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to predict\n",
    "texts = [\n",
    "    \"The local high school soccer team triumphed in the state championship, securing victory with a last-second winning goal.\",\n",
    "    \"DataCore is set to acquire startup InnovateAI for $2 billion, aiming to enhance its position in the artificial intelligence market.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer needs to be loaded sepparetly for this\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(predict(texts, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Method 2: Versatile Loading with Custom Handling\n",
    "\n",
    "This alternate method is more versatile and can handle different types of models. It's particularly useful when you're working with a variety of models or when the environment requires a more customized approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load custom model\n",
    "model_name = \"agnews-transformer\"\n",
    "model_version = \"1\"  # or \"production\", \"pstaging\"\n",
    "model_version_details = client.get_model_version(name=model_name, version=model_version)\n",
    "\n",
    "run_id = model_version_details.run_id\n",
    "artifact_path = model_version_details.source\n",
    "\n",
    "# Construct the model URI\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "model_path = \"models/agnews_transformer\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "client.download_artifacts(run_id, artifact_path, dst_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "custom_model = AutoModelForSequenceClassification.from_pretrained(\"models/agnews_transformer/custom_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/agnews_transformer/custom_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the inference\n",
    "print(predict(texts, custom_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Model Versioning with MLflow\n",
    "\n",
    "One of the powerful features of MLflow is its ability to manage multiple versions of models. In this section, we log new iterations of our model to showcase this versioning capability. By setting a new experiment and logging models under different run names, we effectively create multiple versions of the same model. This is a crucial aspect of MLOps, as it allows for tracking the evolution of models over time, comparing different iterations, and systematically managing the model lifecycle. We demonstrate this by logging two additional iterations of our model, tagged as \"iteration2\" and \"iteration3\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some new models for versioning demonstration\n",
    "mlflow.set_experiment(\"sequence_classification\")\n",
    "\n",
    "# Log a new model as iteration 2\n",
    "with mlflow.start_run(run_name=\"iteration2\"):\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "# Log another new model as iteration 3\n",
    "with mlflow.start_run(run_name=\"iteration3\"):\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Here, we define the `predict` function to perform inference using the loaded model. This function takes a list of texts, tokenizes them using a pre-trained tokenizer, and then feeds them into the model. The output is the model's prediction, which can be used for various applications such as text classification, sentiment analysis, etc. This step is crucial in demonstrating how a trained model can be utilized for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model version management\n",
    "model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "for version in model_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}\")\n",
    "\n",
    "# Change model stage\n",
    "client.transition_model_version_stage(name=model_name, version=model_version, stage=\"Production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Deleting Models and Versions\n",
    "\n",
    "In some scenarios, you might need to delete specific model versions or even entire registered models from MLflow. This section covers how to perform these deletions. Note that this should be done cautiously, as it cannot be undone. This is particularly useful for maintaining a clean and efficient model registry by removing outdated or unused models and versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific model version\n",
    "client.delete_model_version(name=model_name, version=model_version)\n",
    "\n",
    "# Delete the entire registered model\n",
    "client.delete_registered_model(name=model_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
