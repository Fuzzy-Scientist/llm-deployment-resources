{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Transformers and Dynamic Batching üöÄ\n",
    "\n",
    "**Objectives:**\n",
    "- üìö Learn to use pretrained models/tokenizers from Hugging Face.\n",
    "- ‚úçÔ∏è Generate text for prompts.\n",
    "- üßë‚Äçüî¨ Explore batch and dynamic batch text generation.\n",
    "- üèé Optimize text generation efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports üõ†\n",
    "\n",
    "**Imports:**\n",
    "- `transformers`: For models & tokenizers.\n",
    "- `datasets`: Easy data access.\n",
    "- `torch`: Tensor operations.\n",
    "- `tqdm`: Progress bars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer üì¶\n",
    "\n",
    "**Key Steps:**\n",
    "- Load model (`TheFuzzyScientist/diabloGPT_open-instruct`).\n",
    "- Load tokenizer (`microsoft/DialoGPT-medium`).\n",
    "- Set tokenizer padding to `eos_token`.\n",
    "- Enable GPU acceleration (`cuda`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheFuzzyScientist/diabloGPT_open-instruct\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation üìà\n",
    "\n",
    "**Process:**\n",
    "- Use `hakurei/open-instruct-v1` dataset.\n",
    "- Convert to pandas DataFrame for easier handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"hakurei/open-instruct-v1\", split=\"train\")\n",
    "dataset = dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Functions ‚úíÔ∏è\n",
    "\n",
    "**Functions:**\n",
    "- `generate_text`: Single prompt text generation.\n",
    "- `batch_generate_texts`: Batch prompt text generation for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs, max_length=64)\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated[: generated.find(\".\") + 1]\n",
    "\n",
    "\n",
    "generate_text(\"What's the best way to cook chiken breast?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Demo üé≠\n",
    "\n",
    "**Activities:**\n",
    "- Generate text from a single prompt.\n",
    "- Generate texts in batches to observe efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_generate_texts(prompts):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)[\"input_ids\"]\n",
    "    outputs = model.generate(inputs, max_length=64, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "batch_generate_texts(dataset[\"instruction\"][:1].tolist())\n",
    "batch_generate_texts(dataset[\"instruction\"][:20].tolist())\n",
    "batch_generate_texts(dataset[\"instruction\"][:100].tolist())\n",
    "batch_generate_texts(dataset[\"instruction\"][:200].tolist())\n",
    "# batch_generate_texts(dataset[\"instruction\"].sample(200).tolist()) # this might crash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Dynamic Batching for Efficiency ‚öôÔ∏è\n",
    "\n",
    "**Concepts:**\n",
    "- Implement dynamic batching for hardware optimization.\n",
    "- Utilize different batching techniques for performance improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_generate_tokens(tokens):\n",
    "    outputs = model.generate(torch.stack(tokens), max_length=64, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def dynamic_batching(prompts, max_tokens, is_pretokenized=False):\n",
    "    if not is_pretokenized:\n",
    "        tokenized_texts = tokenizer(prompts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(model.device)\n",
    "    else:\n",
    "        tokenized_texts = prompts\n",
    "\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        if current_batch_size + len(tokenized_text) > max_tokens and current_batch:\n",
    "            yield batch_generate_tokens(current_batch)\n",
    "\n",
    "            current_batch, current_batch_size = [], 0\n",
    "\n",
    "        current_batch.append(tokenized_text)\n",
    "        current_batch_size += len(tokenized_text)\n",
    "\n",
    "    # Process final batch\n",
    "    if current_batch:\n",
    "        yield batch_generate_tokens(current_batch)\n",
    "        pass\n",
    "\n",
    "\n",
    "generator = dynamic_batching(dataset[\"instruction\"][:40].tolist() * 1000, 3200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Dynamic Batching and Measuring Performance üìä ‚è±\n",
    "\n",
    "**Steps:**\n",
    "- Apply dynamic batching on a large dataset.\n",
    "- Track performance and efficiency improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def track_time():\n",
    "    start = time.time()  # Record start time\n",
    "    yield\n",
    "    end = time.time()  # Record end time\n",
    "    print(f\"Execution time: {end - start} seconds\")\n",
    "\n",
    "\n",
    "with track_time():\n",
    "    for batch_predictions in tqdm(generator):\n",
    "        continue\n",
    "\n",
    "\n",
    "def sort_batches(prompts, max_tokens):\n",
    "    tokenized_texts = tokenizer(prompts, padding=False)[\"input_ids\"]\n",
    "    sorted_tokens = sorted(tokenized_texts, key=len)\n",
    "\n",
    "    sorted_batches = {}\n",
    "    for sorted_token in sorted_tokens:\n",
    "        length = len(sorted_token)\n",
    "        if length not in sorted_batches:\n",
    "            sorted_batches[length] = []\n",
    "\n",
    "        sorted_batches[length].append(sorted_token)\n",
    "\n",
    "    for length, sorted_batch in sorted_batches.items():\n",
    "        tensor_batch = torch.stack([torch.tensor(sorted_token) for sorted_token in sorted_batch]).to(model.device)\n",
    "        for batch_prediction in dynamic_batching(tensor_batch, max_tokens=max_tokens, is_pretokenized=True):\n",
    "            yield batch_prediction\n",
    "\n",
    "\n",
    "generator = sort_batches(dataset[\"instruction\"][:40].tolist() * 1000, 3200)\n",
    "\n",
    "with track_time():\n",
    "    for batch_predictions in tqdm(generator):\n",
    "        print(len(batch_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps üåà\n",
    "\n",
    "**Achievements:**\n",
    "- Mastered text generation with Transformers.\n",
    "- Learned about batch and dynamic batching efficiencies.\n",
    "\n",
    "**Explore Further:**\n",
    "- Experiment with different models/tokenizers.\n",
    "- Test with various datasets.\n",
    "- Adjust batch size to see performance differences.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
