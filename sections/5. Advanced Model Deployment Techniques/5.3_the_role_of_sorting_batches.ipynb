{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Generation Techniques with Transformers üöÄ\n",
    "\n",
    "In this advanced lab, we dive deeper into efficient text generation techniques using Transformers. We'll explore two batching strategies: normal batching and sorted batching, to optimize our text generation tasks.\n",
    "\n",
    "**Objectives:**\n",
    "- üß∞ Implement advanced text generation functions.\n",
    "- üìä Compare normal vs. sorted batching efficiency.\n",
    "- ‚è± Measure and understand execution time improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports üõ†\n",
    "\n",
    "Before diving into the code, let's ensure we have all the necessary tools:\n",
    "\n",
    "- `transformers` & `datasets`: For our model and data.\n",
    "- `torch`: For tensor operations.\n",
    "- `tqdm`: For progress tracking.\n",
    "- `contextlib` & `time`: For measuring execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import contextmanager\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Tracking Utility ‚è±\n",
    "\n",
    "To compare the efficiency of our batching strategies, we'll use a context manager to track the execution time:\n",
    "\n",
    "- **Purpose:** Measure the time it takes to execute a block of code.\n",
    "- **Output:** Prints the execution time in seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def track_time():\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    print(f\"Execution time: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Setup üß©\n",
    " \n",
    "Setting up our model and tokenizer is crucial for text generation:\n",
    "\n",
    "- **Model:** \"TheFuzzyScientist/diabloGPT_open-instruct\" for instructive text generation.\n",
    "- **Tokenizer:** \"microsoft/DialoGPT-medium\" with padding adjusted.\n",
    "- **Device:** Utilize CUDA for GPU acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"TheFuzzyScientist/diabloGPT_open-instruct\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Initial Tokenization üìö\n",
    "\n",
    "We'll work with a sample dataset for text generation tasks:\n",
    "\n",
    "- **Dataset:** \"hakurei/open-instruct-v1\" converted to a pandas DataFrame.\n",
    "- **Initial Tokenization:** Convert a sample of prompts to input IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"hakurei/open-instruct-v1\", split=\"train\")\n",
    "dataset = dataset.to_pandas()\n",
    "\n",
    "prompts = dataset[\"instruction\"].sample(4).tolist()\n",
    "inputs = tokenizer(prompts, padding=True)[\"input_ids\"]\n",
    "\n",
    "# print('\\n\\n'.join(tokenizer.batch_decode(inputs)))\n",
    "print(\"\\n\\n\".join(tokenizer.batch_decode(inputs)).replace(tokenizer.eos_token, \"[PAD]\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Batching Method üîÑ\n",
    "\n",
    "Normal batching processes prompts in fixed-size batches:\n",
    "\n",
    "- **Chunker Function:** Splits our data into specified batch sizes.\n",
    "- **Batch Generation:** Generates text for each batch of tokens.\n",
    "- **Predict Function:** Orchestrates the batching and generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal batching\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos : pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "def batch_generate_tokens(tokens):\n",
    "    outputs = model.generate(tokens, max_new_tokens=64, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def predict_batch(prompts, batch_size):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)[\"input_ids\"]\n",
    "\n",
    "    for batch in chunker(inputs, batch_size):\n",
    "        yield batch_generate_tokens(batch.to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Normal Batching  ‚ö°\n",
    "\n",
    "Let's generate text using the normal batching method:\n",
    "\n",
    "- **Process:** Tokenize prompts, generate text in batches, and track execution time.\n",
    "- **Observation:** Note the time it takes to process 3000 prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = dataset[\"instruction\"].sample(3000).tolist()\n",
    "\n",
    "with track_time():\n",
    "    for batch_prediction in tqdm(predict_batch(prompts, 32)):\n",
    "        print(len(batch_prediction))\n",
    "        \n",
    "# Execution time: 137.19s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted Batching Method  üî¢\n",
    "\n",
    "Sorted batching aims to improve efficiency by grouping prompts of similar lengths:\n",
    "\n",
    "- **Strategy:** Sort prompts by length and batch accordingly.\n",
    "- **Benefits:** Reduces padding, potentially speeding up computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted Batching\n",
    "def predict_sorted_batches(prompts, max_batch_size):\n",
    "    inputs = tokenizer(prompts, padding=False, truncation=True, max_length=512)[\"input_ids\"]\n",
    "\n",
    "    sorted_tokens = sorted(inputs, key=len)\n",
    "    sorted_batches = {}\n",
    "    for sorted_input in sorted_tokens:\n",
    "        if not len(sorted_input):\n",
    "            continue\n",
    "\n",
    "        length = len(sorted_input)\n",
    "        if length not in sorted_batches:\n",
    "            sorted_batches[length] = []\n",
    "\n",
    "        sorted_batches[length].append(sorted_input)\n",
    "\n",
    "    for length, sorted_batch in sorted_batches.items():\n",
    "        for batch in chunker(sorted_batch, max_batch_size):\n",
    "            tensor_batch = torch.tensor(batch).to(model.device)\n",
    "            yield batch_generate_tokens(tensor_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Sorted Batching üöÄ\n",
    "\n",
    "Applying the sorted batching method:\n",
    "\n",
    "- **Execution:** Similar to normal batching but with sorted prompts.\n",
    "- **Comparison:** Observe the execution time difference from normal batching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with track_time():\n",
    "    for batch_prediction in tqdm(predict_sorted_batches(prompts, 32)):\n",
    "        print(len(batch_prediction))\n",
    "\n",
    "# Execution time: 72.74s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps üåà\n",
    "\n",
    "Through this lab, we've explored advanced batching techniques for text generation with Transformers. We saw firsthand how sorted batching can significantly reduce execution time compared to normal batching.\n",
    "\n",
    "**Encouraged Next Steps:**\n",
    "- ü§ñ Experiment with different models and datasets.\n",
    "- üìê Adjust batch sizes and observe the impact on performance.\n",
    "- üîÑ Explore other optimization techniques for text generation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
