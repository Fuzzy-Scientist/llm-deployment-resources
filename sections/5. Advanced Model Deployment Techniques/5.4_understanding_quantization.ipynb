{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization for Efficient Text Generation üöÄ\n",
    "\n",
    "In this lab, we'll explore model quantization using ctranslate2 and its impact on text generation efficiency. Quantization reduces model size and speeds up inference, crucial for deploying models in resource-constrained environments.\n",
    "\n",
    "**Objectives:**\n",
    "- üì¶ Understand the basics of model quantization.\n",
    "- ‚öñÔ∏è Quantize a pre-trained model for efficient text generation.\n",
    "- ‚è± Compare execution times before and after quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports  üõ†\n",
    "\n",
    "First, let's get our workspace ready with all the necessary tools:\n",
    "\n",
    "- `ctranslate2`: For model conversion and quantization.\n",
    "- `transformers` & `datasets`: For our model, tokenizer, and data.\n",
    "- `torch`: For tensor operations.\n",
    "- `tqdm`: Visual progress indication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ctranslate2\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from ctranslate2.converters import TransformersConverter\n",
    "from ctranslate2 import Generator\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def track_time():\n",
    "    start = time.time()  # Record start time\n",
    "    yield\n",
    "    end = time.time()  # Record end time\n",
    "    print(f\"Execution time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Setup  üß©\n",
    "\n",
    "Before quantization, we need to load and prepare our model and tokenizer:\n",
    "\n",
    "- **Model:** \"TheFuzzyScientist/diabloGPT_open-instruct\" for instructive text generation.\n",
    "- **Tokenizer:** Adjusted for our model's needs.\n",
    "- **Device:** Using CUDA for GPU acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"TheFuzzyScientist/diabloGPT_open-instruct\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quantization  ‚öñÔ∏è\n",
    "\n",
    "Quantizing our model to reduce its size and improve inference speed:\n",
    "\n",
    "- **Conversion & Quantization:** Using `TransformersConverter` for ctranslate2 format conversion with float16 quantization.\n",
    "- **Output:** Quantized model ready for efficient text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to CTranslate2\n",
    "model.save_pretrained(\"models/gpt-instruct\")\n",
    "tokenizer.save_pretrained(\"models/gpt-instruct\")\n",
    "\n",
    "converter = TransformersConverter(\"models/gpt-instruct\")\n",
    "out_path = converter.convert(output_dir=\"models/gpt-instruct-quant\", quantization=\"float16\")\n",
    "\n",
    "generator = Generator(\"models/gpt-instruct-quant\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation üìö\n",
    "\n",
    "Loading and preparing a dataset for our text generation tasks:\n",
    "\n",
    "- **Dataset:** \"hakurei/open-instruct-v1\", a rich source for instructive prompts.\n",
    "- **Sampling:** Selecting 3000 random samples for our experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"hakurei/open-instruct-v1\", split=\"train\")\n",
    "dataset = dataset.to_pandas()\n",
    "\n",
    "prompts = dataset[\"instruction\"].sample(3000, random_state=42).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Batching Method üîÑ\n",
    "\n",
    "Using the original model, we'll generate text in batches to establish a baseline for performance:\n",
    "\n",
    "- **Chunker:** Splits prompts into manageable batch sizes.\n",
    "- **Batch Generation:** Generates text for each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normal batching\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos : pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "def batch_generate_tokens(tokens):\n",
    "    outputs = model.generate(tokens, max_length=256, pad_token_id=tokenizer.eos_token_id, num_beams=2, repetition_penalty=1.5)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def predict_batch(prompts, batch_size):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)[\"input_ids\"]\n",
    "\n",
    "    for batch in chunker(inputs, batch_size):\n",
    "        yield batch_generate_tokens(batch.to(model.device))\n",
    "\n",
    "\n",
    "with track_time():\n",
    "    for batch_prediction in tqdm(predict_batch(prompts, 32)):\n",
    "        continue\n",
    "\n",
    "# Execution time: 242.11289978027344 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Model Batching üéØ\n",
    "\n",
    "Switching to our quantized model for more efficient text generation:\n",
    "\n",
    "- **CTRANS Tokenization:** Adjusting tokenization for ctranslate2 input.\n",
    "- **Batch Generation:** Utilizing the quantized model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTranslate2 batching with quantized model\n",
    "def batch_generate_ctrans(prompts, batch_size):\n",
    "    inputs = [tokenizer.tokenize(prompt, truncation=True, max_length=128) for prompt in prompts]\n",
    "\n",
    "    results = generator.generate_batch(inputs, max_length=256, max_batch_size=batch_size, beam_size=2, repetition_penalty=1.5)\n",
    "\n",
    "    result_ids = [res.sequences_ids[0] for res in results]\n",
    "    return tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Quantized Model üöÄ\n",
    "\n",
    "Finally, let's see the performance improvement with our quantized model:\n",
    "\n",
    "- **Execution:** Generate text with the quantized model.\n",
    "- **Comparison:** Observe the reduction in execution time versus the unquantized model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "with track_time():\n",
    "    batch_generate_ctrans(prompts, 32)\n",
    "\n",
    "# Execution time: 150.97192573547363 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps üåà\n",
    "\n",
    "We've successfully quantized a text generation model and demonstrated significant improvements in efficiency. This showcases the power of model quantization for deploying NLP models in production.\n",
    "\n",
    "**Encouraged Next Steps:**\n",
    "- ü§ñ Try quantizing different models.\n",
    "- üìä Compare quantization effects on various model sizes.\n",
    "- üîç Explore further optimizations for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
